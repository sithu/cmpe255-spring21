# -*- coding: utf-8 -*-
"""Copy of cmpe-255-assg-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AN8nqJj357TckKXv8X7HUuDCGUnudg7-
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.linear_model import LinearRegression
# Input data files are available in the read-only "../input/" directory
# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

name= ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
file = "housing.csv"
df=pd.read_csv(file,delim_whitespace=True,names=name)
df.head()

#     ZN: proportion of residential land zoned for lots over 25,000 sq.ft.
#     INDUS: proportion of non-retail business acres per town
#     CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
#     NOX: nitric oxides concentration (parts per 10 million)
#     1https://archive.ics.uci.edu/ml/datasets/Housing
#     123
#     20.2. Load the Dataset 124
#     RM: average number of rooms per dwelling
#     AGE: proportion of owner-occupied units built prior to 1940
#     DIS: weighted distances to ﬁve Boston employment centers
#     RAD: index of accessibility to radial highways
#     TAX: full-value property-tax rate per $10,000
#     PTRATIO: pupil-teacher ratio by town 
#     12. B: 1000(Bk−0.63)2 where Bk is the proportion of blacks by town 
#     13. LSTAT: % lower status of the population
#     MEDV: Median value of owner-occupied homes in $1000s
#     We can see that the input attributes have a mixture of units.

row,col=df.shape
print(f"Table has {row}rows and {col} columns" )
df.info()

df.corr()

#Using 
from sklearn.linear_model import LinearRegression

df.describe()
x = df[['LSTAT']].to_numpy()
y = df[['MEDV']].to_numpy()

print(x.shape)
print(y.shape)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(x)
#y=scaler.fit(y)
x=scaler.transform(x)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 22)
model = LinearRegression()
model.fit(x_train, y_train)
from sklearn.metrics import mean_squared_error, r2_score 
prediction=model.predict(x_test)
Rsquared = r2_score(y_test, prediction)
RMSE = np.sqrt(mean_squared_error(y_test, prediction))
print("RMSE:",RMSE)
print("R^2: ",Rsquared)
print(x_train.shape)
print(y_train.shape)
import matplotlib.pyplot as plt
plt.scatter(x_train,y_train)
plt.plot(x_train,model.predict(x_train))

from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
polynomial = PolynomialFeatures(degree=2)
model_2 = make_pipeline(polynomial,model)
model_2.fit(x_train,y_train)
prediction_2=model_2.predict(x_test)
Rsquared = r2_score(y_test, prediction_2)
RMSER = np.sqrt(mean_squared_error(y_test, prediction_2))
print("RMSE:",RMSE)
print("R^2: ",Rsquared)
plt.figure()
plt.scatter(x_train,y_train,s=15)
plt.plot(x_train,model_2.predict(x_train),color="r")
plt.title("Polynomial regression with degree "+str(2))
plt.show()

from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
polynomial = PolynomialFeatures(degree=20)
model_2 = make_pipeline(polynomial,model)
model_2.fit(x_train,y_train)
prediction_2=model_2.predict(x_test)
Rsquared = r2_score(y_test, prediction_2)
RMSE = np.sqrt(mean_squared_error(y_test, prediction_2))
print("RMSE:",RMSE)
print("R^2: ",Rsquared)
plt.figure()
plt.scatter(x_train,y_train,s=15)
plt.plot(x_train,model_2.predict(x_train),color="r")
plt.title("Polynomial regression with degree 20 "+str(2))
plt.show()

x2 = df[['LSTAT','RM',"PTRATIO"]].to_numpy()
y2 = df[['MEDV']].to_numpy()

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(x2)
#y=scaler.fit(y)
x2=scaler.transform(x2)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x2,y,test_size=0.2,random_state = 22)
model = LinearRegression()
model.fit(x_train, y_train)
from sklearn.metrics import mean_squared_error, r2_score 
prediction=model.predict(x_test)
Rsquared  = r2_score(y_test, prediction)
RMSE = np.sqrt(mean_squared_error(y_test, prediction))
print("RMSE:",RMSE)
print("R^2: ",Rsquared)
print(x_train.shape)
print(y_train.shape)

